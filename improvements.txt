A major limitation that a user is likely to face is due to the system's dependance on NLTK.
This happens when we cross check the rule-based stem generation of nouns and verbs, to verify that the rules are indeed accurate.
However, this is not only a process that is resource consuming, but it is also reliant on the particular dictionary set of words that the Brown corpus has. Whilst this is an extremely big corpus, verbs like (washes, dresses, fizzes, dazes, analyses, boxes, bathes, etc.) do not have any matches in the corpus in either their infinitive or third-person singular word, and therefore do not pass the test.
A way we could solve this is to let the NLTK library do the stemming process for us, in the same was that we let it do the POS tagging in part C. There are functional and more efficient stemmer tools in NLTK that would be able to do this, allowing for a boost in performance as well.

Another major limitation in the system results the great frequency of ambiguity that is inherent in natural languages. Sentences like "Who likes a girl who swims and dances?" are marked as ambiguous as there can be two parse trees for the sentence. The "and dances" bit can be applied to "swims" inside the relative clause so we would have "a girl who swims and dances" or it could be an attribute of the "Who", so that we could equivalently have "Who dances and likes a girl who swims"? Obviously in practice, there is almost no ambiguity in this case and the listener would understand that the question asked is really the first case, as the swim and dances both apply to the girl. However, the system is not able to distinguish this. An approach that could be a good first step to solve this would be to create have a probabilistic context-free grammar applied to the system. The percentages would come from the analysis (through machine learning or other methods) of one or multiple corpora to see which frequent patterns sentences tend to have in natural languages. Statistical parsing allows us to have, in case of ambiguity of multiple parse trees originated from the sentence, a way of choosing the parse tree that is more likely to occur.

A further step that can be taken from implementing a PCFG, would be to make it lexicalised. PCFGs lack sensitivity to lexical information, making it unable to be reactive to particular lexical constructions associated to specific words.
Having a lexicalised PCFG would solve a more complex layer of ambiguity in the language. It is possible to train a lexicalised PCFG from a set of training examples from the NLTK library.

An easy extension the system that could be implemented in a straightforward way, would be to extend transitive verbs in statements to handle not only proper nouns, but any noun phrase. In fact, currently, sentences like "John likes a girl" are not accepted by the system, and any user would find it natural to have such a statement to be inputted into the system. To be able to accept these sentences we just slightly change the process_statement() function in statements.py to add the lexicon the stem of the noun used as a subject, instead of returning the error "w isn't a proper name".
